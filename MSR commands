docker run tuvistavie/bigcode-tools ls
export BIGCODE_WORKSPACE=$HOME/bigcode-workspace
mkdir -p $BIGCODE_WORKSPACE
mkdir -p $BIGCODE_WORKSPACE/generated-data
alias docker-bigcode='docker run -p 6006:6006 -v $BIGCODE_WORKSPACE:/bigcode-tools/workspace tuvistavie/bigcode-tools'
docker-bigcode bigcode-fetcher search --language=java --user=apache --keyword=commons --stars='>20' -o workspace/generated-data/java-projects.json
docker-bigcode bigcode-fetcher download -i workspace/generated-data/java-projects.json -o workspace/generated-data/repositories
docker-bigcode bigcode-astgen-java --batch -o workspace/generated-data/java-asts 'workspace/generated-data/repositories/**/*.java'
docker-bigcode bigcode-ast-tools visualize-ast workspace/generated-data/java-asts.json -i 0 --no-open -o workspace/generated-data/ast0.png

docker-bigcode bigcode-ast-tools generate-vocabulary --strip-identifiers -o workspace/generated-data/java-vocabulary.tsv workspace/generated-data/java-asts.json

mkdir $BIGCODE_WORKSPACE/generated-data/java-skipgram-data
docker-bigcode bigcode-ast-tools generate-skipgram-data -v workspace/generated-data/java-vocabulary.tsv --ancestors-window-size 2 --children-window-size 0 --without-siblings -o workspace/generated-data/java-skipgram-data/skipgram-data workspace/generated-data/java-asts.json

docker-bigcode sh -c "bigcode-embeddings train -o workspace/generated-data/java-embeddings --vocab-size=75 --emb-size=50 --optimizer=gradient-descent --batch-size=64 workspace/generated-data/java-skipgram-data/skipgram-data*"


MODEL_NAME=$(basename $(ls $BIGCODE_WORKSPACE/generated-data/java-embeddings/embeddings.bin-* | tail -n1) ".meta")


docker-bigcode bigcode-embeddings export workspace/generated-data/java-embeddings/$MODEL_NAME  -o workspace/generated-data/embeddings.npy

###############
for generating actual embeddings
mkdir $BIGCODE_WORKSPACE/dataset/java-skipgram-data
docker-bigcode bigcode-ast-tools generate-skipgram-data -v workspace/dataset/java-vocab.tsv --ancestors-window-size 2 --children-window-size 1 --without-siblings -o workspace/dataset/java-skipgram-data/skipgram-data workspace/dataset/java-asts.json
mkdir $BIGCODE_WORKSPACE/dataset/python-skipgram-data
docker-bigcode bigcode-ast-tools generate-skipgram-data -v workspace/dataset/python-vocab.tsv --ancestors-window-size 2 --children-window-size 1 --without-siblings -o workspace/dataset/python-skipgram-data/skipgram-data workspace/dataset/python-asts.json

### train the skip gram model 
docker-bigcode sh -c "bigcode-embeddings train -o workspace/dataset/java-embeddings --vocab-size=10000 --emb-size=50 --optimizer=gradient-descent --batch-size=64 workspace/dataset/java-skipgram-data/skipgram-data*"
JAVA_MODEL_NAME=$(basename $(ls $BIGCODE_WORKSPACE/dataset/java-embeddings/embeddings.bin-* | tail -n1) ".meta")
docker-bigcode bigcode-embeddings export workspace/dataset/java-embeddings/$JAVA_MODEL_NAME  -o workspace/dataset/java-embeddings.npy

docker-bigcode sh -c "bigcode-embeddings train -o workspace/dataset/python-embeddings --vocab-size=10000 --emb-size=50 --optimizer=gradient-descent --batch-size=64 workspace/dataset/python-skipgram-data/skipgram-data*"
PYTHON_MODEL_NAME=$(basename $(ls $BIGCODE_WORKSPACE/dataset/python-embeddings/embeddings.bin-* | tail -n1) ".meta")
docker-bigcode bigcode-embeddings export workspace/dataset/python-embeddings/$PYTHON_MODEL_NAME  -o workspace/dataset/python-embeddings.npy



